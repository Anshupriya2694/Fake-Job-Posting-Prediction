{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /Users/anshupriyasrivastava/Desktop/Udacity/Udacity ML Nanodegree/Fake-Job-Posting-Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings = pd.read_csv('data/fake_job_postings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = fake_job_postings.corr()\n",
    "sns.heatmap(corr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='fraudulent', data=fake_job_postings);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sns_countplot(feature):\n",
    "    sns.countplot(x=feature, data=fake_job_postings, hue=\"fraudulent\",\n",
    "              order=fake_job_postings[feature].value_counts().iloc[:10].index)\n",
    "    plt.xticks(rotation=90)\n",
    "    title = feature + ' fake job count'\n",
    "    plt.title('Location Fake Jobs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_countplot('location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_countplot('employment_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_countplot('required_experience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_countplot('required_education')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_ratio = round(fake_job_postings[fake_job_postings.fraudulent == 1].groupby('location').location.count()/fake_job_postings[fake_job_postings.fraudulent == 0].groupby('location').location.count(), 2)\n",
    "location_ratio = pd.DataFrame({'location':location_ratio.index, 'ratio':location_ratio.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings = fake_job_postings.merge(location_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings.ratio.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_ratio_dict = {}\n",
    "\n",
    "for key, value in zip(fake_job_postings.location, fake_job_postings.ratio):\n",
    "    location_ratio_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "location_ratio_df = pd.DataFrame()\n",
    "location_ratio_df_ratio = []\n",
    "location_ratio_df_country = []\n",
    "location_ratio_df_state = []\n",
    "location_ratio_df_city = []\n",
    "for key, value in zip(location_ratio_dict.keys(), location_ratio_dict.values()):\n",
    "    keys = key.split(',')\n",
    "    print(keys, value)\n",
    "    # for v in keys:\n",
    "    #     clean_key = v.strip()\n",
    "    #     location_ratio_df_country.append(clean_key[0])\n",
    "    #     location_ratio_df_state.append(clean_key[1])\n",
    "    #     location_ratio_df_city.append(clean_key[2])\n",
    "    # location_ratio_df_ratio.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_ratio = location_ratio[location_ratio.ratio >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=location_ratio.sort_values(by='ratio'), x='location', y='ratio')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Fake to Real Job Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_axis = fake_job_postings[fake_job_postings.function.isna()][['fraudulent', 'function']]\n",
    "y_axis = y_axis.fraudulent.value_counts()\n",
    "y_axis.plot(kind='bar')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_count(feature, title='None'):\n",
    "    y_axis = fake_job_postings[fake_job_postings[feature].isna()][['fraudulent', feature]]\n",
    "    y_axis = y_axis.fraudulent.value_counts()\n",
    "    y_axis.plot(kind='bar')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Category')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count('function', 'Function Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count('company_profile', 'Company Profile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count('required_education', 'required_education')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count('industry', 'Industry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings[(fake_job_postings.telecommuting == 0) & (fake_job_postings.fraudulent == 1)][['telecommuting', 'fraudulent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "telecommuting_list = []\n",
    "has_company_logo_list = []\n",
    "\n",
    "for idx, tel, logo in zip(range(len(fake_job_postings)), fake_job_postings.telecommuting, fake_job_postings.has_company_logo):\n",
    "    if fake_job_postings.fraudulent[idx] == 1:\n",
    "        telecommuting_list.append(tel)\n",
    "        has_company_logo_list.append(logo)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "telecommuting_logo_df = pd.DataFrame({'telecommuting':telecommuting_list, 'has_company_logo':has_company_logo_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_count = 0\n",
    "\n",
    "for fraud, tel, logo in zip(fake_job_postings.fraudulent, fake_job_postings.telecommuting, fake_job_postings.has_company_logo):\n",
    "    if (tel == 0 and logo == 0):\n",
    "        if (fraud == 1):\n",
    "            fake_count +=1\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "print(fake_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fake_job_postings[fake_job_postings.fraudulent == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_count/len(fake_job_postings[fake_job_postings.fraudulent == 1]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings.fillna(\" \",inplace = True)\n",
    "\n",
    "fake_job_postings['text'] =  fake_job_postings['title'] + ' ' + fake_job_postings['location'] + ' ' + fake_job_postings['company_profile'] + ' ' + \\\n",
    "        fake_job_postings['description'] + ' ' + fake_job_postings['requirements'] + ' ' + fake_job_postings['benefits'] + ' ' + \\\n",
    "        fake_job_postings['required_experience'] + ' ' + fake_job_postings['required_education'] + ' ' + fake_job_postings['industry'] + ' ' + fake_job_postings['function']\n",
    "\n",
    "\n",
    "fake_job_postings.drop(['job_id', 'department', 'salary_range', 'title','location','department','company_profile','description','requirements','benefits','employment_type','required_experience','required_education','industry','function'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings['character_count'] = fake_job_postings.text.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings[fake_job_postings.fraudulent==0].character_count.plot(bins=35, kind='hist', color='blue', \n",
    "                                       label='Real', alpha=0.8)\n",
    "fake_job_postings[fake_job_postings.fraudulent==1].character_count.plot(kind='hist', color='red', \n",
    "                                       label='Fake', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Character Count\");"
   ]
  },
  {
   "source": [
    "## Text Data Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pattern_url = \"#URL\\_d\"\n",
    "\n",
    "fake_job_postings_fraud_only = fake_job_postings[fake_job_postings.fraudulent == 1]\n",
    "fake_job_postings_not_fraud = fake_job_postings[fake_job_postings.fraudulent == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lines_fraud = [regexp_tokenize(t, \"\\w+\") for t in fake_job_postings_fraud_only.text]\n",
    "line_num_words_fraud = [len(t_line) for t_line in tokenized_lines_fraud]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lines_not_fraud = [regexp_tokenize(t, \"\\w+\") for t in fake_job_postings_not_fraud.text]\n",
    "line_num_words_not_fraud = [len(t_line) for t_line in tokenized_lines_not_fraud]\n",
    "plt.hist(line_num_words_not_fraud, label='Real', color='blue', alpha=0.6)\n",
    "plt.hist(line_num_words_fraud, label='Fake', color='red')\n",
    "plt.title('Word Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation removal\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings['text'] = fake_job_postings['text'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings.text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toeknization\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "fake_job_postings_tokenized = fake_job_postings\n",
    "fake_job_postings_tokenized['text'] = fake_job_postings_tokenized['text'].apply(lambda x: tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings_tokenized['text'] = fake_job_postings_tokenized['text'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmentizing\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [lemmatizer.lemmatize(i) for i in text]\n",
    "    return lem_text\n",
    "\n",
    "\n",
    "fake_job_postings_tokenized['text'] = fake_job_postings_tokenized['text'].apply(lambda x: word_lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def word_stemmer(text):\n",
    "    stem_text = \" \".join([stemmer.stem(i) for i in text])\n",
    "    return stem_text\n",
    "\n",
    "\n",
    "fake_job_postings_tokenized['text'] = fake_job_postings_tokenized['text'].apply(lambda x: word_stemmer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'[0-9]'\n",
    "\n",
    "for text in fake_job_postings_tokenized['text']:\n",
    "    fake_job_postings_tokenized['text'] = re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(fake_job_postings[fake_job_postings.fraudulent == 0].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(fake_job_postings[fake_job_postings.fraudulent == 1].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_job_postings.to_csv('data/fake_job_postings_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}